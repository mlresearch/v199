---
title: A Theory for Knowledge Transfer in Continual Learning
abstract: Continual learning of a stream of tasks is an active area in deep neural
  networks. The main challenge investigated has been the phenomenon of catastrophic
  forgetting or interference of newly acquired knowledge with knowledge from previous
  tasks. Recent work has investigated forward knowledge transfer to new tasks. Backward
  transfer for improving knowledge gained during previous tasks has received much
  less attention. There is in general limited understanding of how knowledge transfer
  could aid tasks learned continually. We present a theory for knowledge transfer
  in continual supervised learning, which considers both forward and backward transfer.
  We aim at understanding their impact for increasingly knowledgeable learners. We
  derive error bounds for each of these transfer mechanisms. These bounds are agnostic
  to specific implementations (e.g. deep neural networks). We demonstrate that, for
  a continual learner that observes related tasks, both forward and backward transfer
  can contribute to an increasing performance as more tasks are observed.
video: https://youtu.be/rs119ZMh7_0
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: prado22a
month: 0
tex_title: A Theory for Knowledge Transfer in Continual Learning
firstpage: 647
lastpage: 660
page: 647-660
order: 647
cycles: false
bibtex_author: Prado, Diana Benavides and Riddle, Patricia
author:
- given: Diana Benavides
  family: Prado
- given: Patricia
  family: Riddle
date: 2022-11-29
address:
container-title: Proceedings of The 1st Conference on Lifelong Learning Agents
volume: '199'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 29
pdf: https://proceedings.mlr.press/v199/prado22a/prado22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
